# Test runs

## Description
Now it's time to integrate our tester ai agent to our platform, we want it to start testing the test cases, but before that, we should prepare the requirements for it

## Requirements
- We should have a test run model, which has a many to many relation with test cases and a status field that talk about if the test run is done, started, or waiting
- On the pivot model testrun-testcase we should store the ai agent logs and the run status (created, in progress, success, failed) and the result, so we can't use a simple many to many relation there, we should have a custom table
- On running test agent loop, right now we're logging everything which is good for debugging, we should change those log levels to debug
- While changing those log level to debug, we should save those to db so be able to show it on frontend later, so we need a `logs` column for pivot table and it should store agent and tools logs as a string, like this:
call_tool_x(...)
\[tool x result\] some result
\[agent\] Some stuff
...

## Flow:
We need a single entrypoint, like a single service function, that we feed it with a testrun-testcase pivot, it will create a new container (with the testrun-testcase id as the name or included on the name), run the agent on it to test the test case, update the record, clean the container, done

## Constraint
Implement the django admin as well, so I can create the test run and test case from admin, then will trigger it using management commands