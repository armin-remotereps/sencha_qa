# Sub-Agent Architecture

## Problem

The current agent loop runs a single flat context for the entire test case. A 10-step test with screenshots and command outputs exhausts the context window (even 32k tokens) by iteration 6-7. The context summarizer helps but is lossy — the agent forgets what it already did and repeats actions or gets confused.

## Solution

Replace the flat agent loop with a two-tier **orchestrator + sub-agent** architecture:

- **Orchestrator** (smart model, large context): reads the full test case, decomposes it into sub-tasks, dispatches sub-agents one at a time, tracks pass/fail per sub-task, handles failure recovery, and produces the final verdict.
- **Sub-agent** (fast/strict model, small context): receives ONE focused sub-task + a brief state description, executes it using the full tool set, and returns a structured result (pass/fail + summary).

Each sub-agent gets a **clean, isolated context** — no pollution from previous steps. The orchestrator's context stays lean because it only stores sub-task results (a few lines each), not the full tool call history.

## Architecture

```
execute_test_run_test_case (projects/services.py)
  └── run_orchestrator (agents/services/orchestrator.py)
        ├── Phase 1: PLAN — Decompose test case into sub-tasks
        │     Orchestrator reads preconditions + steps + expected results
        │     Produces ordered list of sub-tasks (may split compound steps)
        │
        ├── Phase 2: EXECUTE — Run sub-tasks sequentially
        │     For each sub-task:
        │       ├── Build state context (what happened so far)
        │       ├── run_sub_agent (agents/services/sub_agent.py)
        │       │     ├── Fresh context: system prompt + sub-task + state
        │       │     ├── Tool loop (same tools as current agent)
        │       │     └── Returns: SubTaskResult (status, summary, error)
        │       ├── Record result in orchestrator context
        │       └── If failed: orchestrator decides recover or stop
        │
        └── Phase 3: VERDICT — Final pass/fail for the test case
```

## Orchestrator Design

### Model & Context

| Setting | Value | Reason |
|---|---|---|
| Model | `ai/qwen3:32B-Q8_K_XL` | Needs strong reasoning for planning and failure recovery |
| Context | 32k tokens | Holds system prompt + test case + all sub-task results |
| Max output tokens | 4096 | Enough for planning response |
| Temperature | 0.1 | Precise planning |

### What the Orchestrator Does

1. **Receives** the full test case (preconditions, steps, expected results)
2. **Plans** by breaking it into sub-tasks via a structured LLM call. The orchestrator can:
   - Keep existing test steps as-is
   - Split compound steps (e.g., "Install JDK, verify version, configure JAVA_HOME" → 3 sub-tasks)
   - Add implicit steps (e.g., "open browser" before a web navigation step)
3. **Dispatches** sub-tasks one at a time, passing:
   - The sub-task description + expected result
   - A brief **state summary** of what happened so far (e.g., "Browser is on https://example.com/dashboard. User is logged in as admin. JDK 17 installed at /usr/lib/jvm/java-17.")
4. **Handles failures** by classifying them:
   - **Recoverable**: the orchestrator creates a recovery sub-task and retries (e.g., "npm not found" → sub-task: "install Node.js via apt")
   - **Non-recoverable**: the orchestrator stops and fails the test case (e.g., "login page returns 404 — the application is not deployed")
5. **Produces a final verdict** summarizing all sub-task results

### Orchestrator Does NOT

- Call any tools directly (no click, type, screenshot, etc.)
- Have access to the tool set
- See raw tool outputs — only sub-task result summaries

### Orchestrator Prompt Structure

```
SYSTEM: You are a QA test orchestrator. You break test cases into
sub-tasks, dispatch them to executor agents, and decide pass/fail.

You will be called in two modes:

MODE 1 — PLAN:
Given a test case, output a JSON array of sub-tasks.
Each sub-task has: description, expected_result.

MODE 2 — EVALUATE:
Given a sub-task result, decide:
- "continue": proceed to next sub-task
- "recover": create a recovery sub-task (provide it) then retry
- "stop": the failure is non-recoverable, fail the test

USER: <test case>
```

### Orchestrator Output Format (Plan Phase)

```json
{
  "sub_tasks": [
    {
      "description": "Navigate to https://example.com/login",
      "expected_result": "Login page is displayed with email and password fields"
    },
    {
      "description": "Enter admin@example.com in the email field and click Login",
      "expected_result": "Dashboard page loads showing welcome message"
    }
  ]
}
```

### Orchestrator Output Format (Evaluate Phase)

```json
{
  "decision": "continue" | "recover" | "stop",
  "reason": "...",
  "recovery_task": {  // only when decision=recover
    "description": "...",
    "expected_result": "..."
  }
}
```

## Sub-Agent Design

### Model & Context

| Setting | Value | Reason |
|---|---|---|
| Model | `ai/mistral` | Fast, reliable tool calling, cheap |
| Context | 8k tokens | One sub-task never needs more |
| Max output tokens | 2048 | Enough for tool calls + reasoning |
| Temperature | 0.1 | Precise tool calling |
| Max iterations | 15 | Per sub-task safety limit |
| Timeout | 180s | Per sub-task timeout |

### What the Sub-Agent Does

1. **Receives**: sub-task description + expected result + current environment state
2. **Executes** using the full tool set (click, type, screenshot, browser_*, execute_command, etc.)
3. **Verifies** the expected result (takes a screenshot or checks output)
4. **Returns** a structured result: pass/fail + concise summary of what happened

### Sub-Agent Prompt

```
SYSTEM: You are a strict QA step executor. You execute ONE test step
and report the result honestly.

[same tool descriptions as current agent]
[same environment context]

RULES:
- Execute the step exactly as described
- After executing, verify the expected result
- Respond with EXACTLY this format when done:
  RESULT: PASS or FAIL
  SUMMARY: <1-3 sentences of what happened>
- If you cannot execute the step, respond with FAIL and explain why

CURRENT STATE:
{state_description}

YOUR TASK:
{sub_task_description}

EXPECTED RESULT:
{expected_result}
```

### Sub-Agent Tool Set

Same as the current agent — all controller, browser, and search tools. No restrictions.

## State Tracking

Between sub-tasks, the orchestrator maintains a **state description** that gets passed to the next sub-agent. This solves the "step 2 depends on step 1" problem.

The state description is built by:
1. Starting with an empty state
2. After each sub-task, appending the sub-agent's summary to the state
3. Optionally, the orchestrator can refine/compress the state if it grows too long

Example state progression:
```
After sub-task 1: "Browser is on https://example.com/login"
After sub-task 2: "User is logged in as admin. Dashboard page is showing."
After sub-task 3: "User is logged in. Navigated to Settings > Profile page."
```

## Recovery Flow

When a sub-agent fails:

```
Orchestrator: "Sub-task 3 failed: npm: command not found"
Orchestrator evaluates: this is recoverable
Orchestrator creates recovery sub-task: "Install Node.js 20 LTS using apt"
  └── Sub-agent executes recovery task
      └── Returns: PASS, "Node.js 20.11 installed at /usr/bin/node"
Orchestrator retries original sub-task 3
  └── Sub-agent executes: "Run npm install in /app directory"
      └── Returns: PASS, "npm install completed, 142 packages installed"
```

Limits:
- Max 1 recovery attempt per sub-task (no infinite recovery loops)
- If recovery sub-task itself fails → non-recoverable, stop test

## New Types

```python
@dataclass(frozen=True)
class SubTask:
    description: str
    expected_result: str

@dataclass(frozen=True)
class SubTaskResult:
    status: Literal["pass", "fail"]
    summary: str
    iterations: int
    error: str | None = None

@dataclass(frozen=True)
class OrchestratorDecision:
    action: Literal["continue", "recover", "stop"]
    reason: str
    recovery_task: SubTask | None = None

@dataclass(frozen=True)
class OrchestratorResult:
    status: Literal["pass", "fail"]
    summary: str
    sub_task_results: tuple[SubTaskResult, ...]
    total_iterations: int
```

## New Settings

```env
# Orchestrator
ORCHESTRATOR_MODEL=ai/qwen3:32B-Q8_K_XL
ORCHESTRATOR_MAX_TOKENS=4096
ORCHESTRATOR_TEMPERATURE=0.1

# Sub-Agent
SUB_AGENT_MODEL=ai/mistral
SUB_AGENT_MAX_TOKENS=2048
SUB_AGENT_TEMPERATURE=0.1
SUB_AGENT_MAX_ITERATIONS=15
SUB_AGENT_TIMEOUT_SECONDS=180

# Safety limits
ORCHESTRATOR_MAX_SUBTASKS=30
ORCHESTRATOR_MAX_RECOVERY_ATTEMPTS=1
```

## New Files

| File | Purpose |
|---|---|
| `agents/services/orchestrator.py` | Orchestrator loop: plan → execute → evaluate → verdict |
| `agents/services/sub_agent.py` | Sub-agent runner: receives sub-task, runs tool loop, returns result |
| `agents/services/orchestrator_prompts.py` | System prompts and prompt builders for orchestrator |
| `agents/services/sub_agent_prompts.py` | System prompts and prompt builders for sub-agent (extracted from current `agent_loop.py`) |

## Changes to Existing Files

| File | Change |
|---|---|
| `projects/services.py` | `execute_test_run_test_case` calls `run_orchestrator` instead of `run_agent` |
| `agents/services/agent_loop.py` | Extract prompt builders into separate files. Keep `_run_agent_loop` as the core tool-calling loop (used by sub-agents). |
| `agents/types.py` | Add `SubTask`, `SubTaskResult`, `OrchestratorDecision`, `OrchestratorResult` |
| `agents/services/dmr_config.py` | Add `build_orchestrator_config`, `build_sub_agent_config` |
| `auto_tester/settings.py` | Add new settings |
| `example.env` | Add new env vars |

## Integration with Existing Systems

### WebSocket Broadcasting

Sub-agent tool calls and results are **still broadcast** via the existing log/screenshot callbacks. The user sees real-time logs from each sub-agent step. The orchestrator's planning and evaluation decisions are also broadcast as log messages.

Log output example:
```
[Orchestrator] Planning: decomposed test case into 5 sub-tasks
[Orchestrator] Sub-task 1/5: Navigate to login page
  [Sub-agent] browser_navigate(url="https://example.com/login")
  [Sub-agent] browser_take_screenshot(question="Is the login page shown?")
  [Sub-agent] RESULT: PASS — Login page displayed with email and password fields
[Orchestrator] Sub-task 2/5: Log in as admin
  [Sub-agent] browser_type(description="email input", text="admin@example.com")
  ...
```

### Context Summarizer

The existing context summarizer still works inside each sub-agent's loop (for the rare case a single sub-task takes many iterations). But with 8k context and 15 max iterations, it should rarely trigger.

### Output Summarizer

Still works — long tool outputs inside sub-agents get summarized before entering the sub-agent's context.

### Vision

Sub-agents use vision (DMR or OpenAI) for screenshot analysis, same as current agent.

## Verification

1. Run a complex multi-step test case (10+ steps) that currently fails due to context overflow
2. Verify the orchestrator correctly decomposes the test case
3. Verify each sub-agent executes its step and returns a structured result
4. Verify failure recovery works (e.g., missing dependency gets installed)
5. Verify non-recoverable failures stop the test
6. Verify WebSocket logs show both orchestrator and sub-agent activity
7. Compare token usage: old (single agent) vs new (orchestrator + sub-agents)
